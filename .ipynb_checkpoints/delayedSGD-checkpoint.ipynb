{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from scipy import sparse\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from random import shuffle\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queue:\n",
    "    def __init__(self, max_len):\n",
    "        self.queue = list()\n",
    "        self.maxlen = max_len\n",
    "        self.len = 0\n",
    "        \n",
    "    def push(self, grad):\n",
    "        \n",
    "        if self.len < self.maxlen:\n",
    "            self.queue.append(grad)\n",
    "            self.len += 1\n",
    "        else:\n",
    "            ret = self.queue.pop(0)\n",
    "            self.queue.append(grad)\n",
    "    \n",
    "    def sample(self, delay):\n",
    "\n",
    "        if delay >= self.len:\n",
    "            delay = 0\n",
    "            return self.queue[0]\n",
    "#         print(delay)\n",
    "        # i-th element in the queue is the i step delayed version of the param\n",
    "        return self.queue[self.len - delay - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\n",
    "    x -= np.max(x, axis=1).reshape(-1,1)\n",
    "    z = np.exp(x)/ np.sum(np.exp(x), axis=1).reshape(-1,1)\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def ReLu(x):\n",
    "    \n",
    "    return np.maximum(x, np.zeros_like(x))\n",
    "\n",
    "\n",
    "def data_partition(num_workers, X_train, Y_train, separate=True):\n",
    "    \n",
    "    size = X_train.shape[0]\n",
    "    ind = list(range(size))\n",
    "    shuffle(ind)\n",
    "    \n",
    "    if separate:\n",
    "        worker_size = int(np.floor(size/num_workers))\n",
    "        data = dict.fromkeys(list(range(num_workers)))\n",
    "\n",
    "        for w in range(num_workers):\n",
    "            data[w] = dict()\n",
    "            if w is not num_workers-1:\n",
    "                data[w][\"X\"] = X_train[ind[w*worker_size: (w+1)*worker_size],:]\n",
    "                data[w][\"Y\"] = Y_train[ind[w*worker_size: (w+1)*worker_size],:]\n",
    "            else:\n",
    "                data[w][\"X\"] = X_train[ind[w*worker_size:],:]\n",
    "                data[w][\"Y\"] = Y_train[ind[w*worker_size:],:]\n",
    "\n",
    "    else:\n",
    "        data = dict.fromkeys(list(range(num_workers)))\n",
    "        for w in range(num_workers):\n",
    "            data[w] = dict()\n",
    "            shuffle(ind)\n",
    "            data[w][\"X\"] = X_train[ind,:]\n",
    "            data[w][\"Y\"] = Y_train[ind,:]\n",
    "\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def unpack_data(data):\n",
    "    \n",
    "    X_train, Y_train = data[\"X\"], data[\"Y\"]\n",
    "    \n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamServer:\n",
    "    def __init__(self, num_workers, max_delay, dim_in, num_classes, \n",
    "                 layers, activation, learning_rate=1e-3, num_iter=10000,\n",
    "                 regularization=1e-2, dataset=\"a9a\", batch_size=100, num_epochs=10):\n",
    "\n",
    "        self.max_delay = max_delay\n",
    "        self.queue = Queue(max_delay + 1)\n",
    "        \n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_workers = num_workers\n",
    "        self.workers = dict.fromkeys(list(range(self.num_workers)))        \n",
    "        \n",
    "        self.X_train = None\n",
    "        self.Y_train = None\n",
    "        self.X_test = None\n",
    "        self.Y_test = None\n",
    "        self.batch_size = batch_size\n",
    "        self.train_batch_counter = 0\n",
    "        self.test_batch_counter = 0\n",
    "        \n",
    "        self.data_partition = self.get_dataset(dataset) \n",
    "    \n",
    "        for i in range(num_workers):\n",
    "            self.workers[i] = Worker(i, self, self.data_partition[i], dim_in,\n",
    "                                     num_classes, layers, activation, regularization)\n",
    "\n",
    "        self.nn = NeuralNet(dim_in, num_classes, layers, activation, regularization)\n",
    "        self.lr = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.lamda = regularization\n",
    "        self.num_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        self.dim_in = dim_in\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.clear_grads()\n",
    "        self.initialize_weights(dim_in, num_classes, layers)\n",
    "        self.log = dict()\n",
    "        \n",
    "           \n",
    "    def get_dataset(self, dataset=\"a9a\"):\n",
    "        \n",
    "        if dataset == \"a9a\":\n",
    "            self.num_classes = 2\n",
    "            X_train, Y_train = load_svmlight_file(\"data/a9a/a9a.txt\")\n",
    "            X_test, Y_test = load_svmlight_file(\"data/a9a/a9a.t\")\n",
    "            X_test = sparse.hstack((X_test, np.zeros((X_test.shape[0],1))))\n",
    "        \n",
    "            X_train = X_train.tocsc()\n",
    "            X_test = X_test.tocsc()\n",
    "            #converting labels from -1,+1 to 0,1\n",
    "            Y_test += 1\n",
    "            Y_train += 1\n",
    "            Y_test /= 2\n",
    "            Y_train /=2\n",
    "            \n",
    "            Y_test = np.eye(self.num_classes)[Y_test.astype(\"int\")]\n",
    "            Y_train = np.eye(self.num_classes)[Y_train.astype(\"int\")]\n",
    "\n",
    "        elif dataset == \"mnist\":\n",
    "            self.num_classes = 10\n",
    "            mnist = fetch_mldata('MNIST original', data_home=\"data/\")\n",
    "            mnist.data = preprocessing.normalize(mnist.data)\n",
    "            target = np.eye(10)[mnist.target.astype(\"int\")]\n",
    "            ind = list(range(target.shape[0]))\n",
    "            shuffle(ind)\n",
    "\n",
    "            X_train = mnist.data[ind[:int(np.floor(target.shape[0]*0.8))],:]\n",
    "            Y_train = target[ind[:int(np.floor(target.shape[0]*0.8))],:]\n",
    "            X_test = mnist.data[ind[int(np.floor(target.shape[0]*0.8)):],:]\n",
    "            Y_test = target[ind[int(np.floor(target.shape[0]*0.8)):],:]\n",
    "\n",
    "            \n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "\n",
    "        data = data_partition(self.num_workers, X_train, Y_train, separate=True)\n",
    "                \n",
    "        return data\n",
    "        \n",
    "        \n",
    "    def clear_grads(self):\n",
    "        prev_dim = self.dim_in\n",
    "        for key, value in enumerate(self.layers):\n",
    "            self.nn.gradients_W[key] = np.zeros((prev_dim, value))\n",
    "            self.nn.gradients_b[key] = np.zeros((1, value))\n",
    "            prev_dim = value\n",
    "        \n",
    "        self.nn.gradients_W[self.num_layers] = np.zeros((prev_dim, self.num_classes))\n",
    "        self.nn.gradients_b[self.num_layers] = np.zeros((1, self.num_classes))\n",
    "\n",
    "\n",
    "    def initialize_weights(self, dim_in, num_classes, layers):\n",
    "        prev_dim = dim_in\n",
    "        for key, value in enumerate(layers):\n",
    "            self.nn.weights[key] = np.random.randn(prev_dim, value)/np.sqrt(prev_dim)\n",
    "            self.nn.bias[key] = np.random.randn(1, value)\n",
    "            prev_dim = value\n",
    "        \n",
    "        self.nn.weights[self.num_layers] = np.random.randn(prev_dim, num_classes)/np.sqrt(prev_dim)\n",
    "        self.nn.bias[self.num_layers] = np.random.randn(1, num_classes)\n",
    "        self.queue.push([self.nn.weights, self.nn.bias])\n",
    "\n",
    "    \n",
    "    def update(self, count):\n",
    "        gamma_t = self.learning_rate(count)\n",
    "        weights = dict()\n",
    "        bias = dict()\n",
    "        for ind in self.nn.weights.keys():\n",
    "            weights[ind] = np.zeros_like(self.nn.weights[ind])\n",
    "            bias[ind] = np.zeros_like(self.nn.bias[ind])\n",
    "\n",
    "            weights[ind] = self.nn.weights[ind] - gamma_t * self.nn.gradients_W[ind]\n",
    "            bias[ind] = self.nn.bias[ind] - gamma_t * self.nn.gradients_b[ind]\n",
    "        \n",
    "        return weights, bias\n",
    "\n",
    "            \n",
    "    def step(self):    \n",
    "        self.clear_grads()\n",
    "        size = 0\n",
    "        loss = 0\n",
    "        accuracy = 0 \n",
    "        for ind, worker in self.workers.items():\n",
    "#             batch_X, batch_Y = worker.get_next_mini_batch()\n",
    "            batch_X, batch_Y = worker.get_next_mini_batch()\n",
    "            grads_W, grads_b = worker.compute_mini_batch_gradients(batch_X, batch_Y)\n",
    "            for layer in range(self.num_layers+1):\n",
    "                self.nn.gradients_W[layer] += grads_W[layer]/self.num_workers\n",
    "                self.nn.gradients_b[layer] += grads_b[layer]/self.num_workers\n",
    "            \n",
    "            temp_loss = worker.nn.compute_loss(batch_X, batch_Y)* batch_Y.shape[0]\n",
    "            pred =  worker.nn.predict(batch_X)\n",
    "            temp_accuracy = np.equal(pred, np.argmax(batch_Y,axis=1)).sum()\n",
    "    \n",
    "            loss += temp_loss\n",
    "            accuracy += temp_accuracy\n",
    "            size += batch_X.shape[0]\n",
    "            \n",
    "        return loss, accuracy, size\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        logs = dict()\n",
    "        data_size = self.workers[0].data_size\n",
    "        num_counts = int(np.floor(data_size/self.batch_size))+ 1\n",
    "        for epoch in range(self.num_epochs):\n",
    "            logs[epoch] = dict()\n",
    "            logs[epoch][\"train_loss\"] = []\n",
    "            logs[epoch][\"train_acc\"] = []\n",
    "            logs[epoch][\"test_loss\"] = []\n",
    "            logs[epoch][\"test_acc\"] = []\n",
    "            running_loss = 0\n",
    "            running_acc = 0\n",
    "            running_size = 0\n",
    "            for _, worker in self.workers.items():\n",
    "                worker.shuffle_data()\n",
    "                worker.reset_batch_counter()\n",
    "\n",
    "            for count in range(1, num_counts):\n",
    "                step_loss, step_accuracy, size = self.step()\n",
    "                weights, bias = self.update(count)\n",
    "                \n",
    "                self.queue.push([weights, bias])\n",
    "                self.nn.weights, self.nn.bias = weights, bias\n",
    "                running_loss += step_loss\n",
    "                running_acc += step_accuracy\n",
    "                running_size += size\n",
    "                #print([self.queue.queue[i][1] for i in range(self.queue.len)],\"\\n\")\n",
    "#                 if count%10 == 0:\n",
    "#                     break\n",
    "#                 if count%100 == 0:\n",
    "#                     loss, accuracy = self.compute_test()\n",
    "\n",
    "#                 print(\"count\", count,\"train_loss\", round(train_loss,2),\n",
    "#                       \"test_loss\",round(loss,2), \"test_accuracy\", round(accuracy,2))\n",
    "            \n",
    "            epoch_train_loss = running_loss / running_size\n",
    "            epoch_train_accuracy = running_acc/ running_size\n",
    "\n",
    "            epoch_loss, epoch_accuracy = self.compute_test()\n",
    "#             epoch_train_loss, epoch_train_accuracy = self.compute_train()\n",
    "#             logs[epoch][\"train_loss\"].append(epoch_train_loss)\n",
    "#             logs[epoch][\"test_loss\"].append(epoch_loss)\n",
    "#             logs[epoch][\"train_acc\"].append(epoch_train_accuracy)\n",
    "#             logs[epoch][\"test_acc\"].append(epoch_accuracy)\n",
    "            logs[epoch][\"train_loss\"] = epoch_train_loss\n",
    "            logs[epoch][\"test_loss\"] = epoch_loss\n",
    "            logs[epoch][\"train_acc\"] = epoch_train_accuracy\n",
    "            logs[epoch][\"test_acc\"] = epoch_accuracy\n",
    "\n",
    "\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "            print(\"epoch\", epoch,\"train_loss\", round(epoch_train_loss,2),\n",
    "                  \"test_loss\",round(epoch_loss,2), \"test_accuracy\", round(epoch_accuracy,2))\n",
    "            \n",
    "        return logs\n",
    "            \n",
    "    def learning_rate(self, count):\n",
    "\n",
    "#         return self.lr\n",
    "        return self.lr/count\n",
    "\n",
    "    def compute_train(self):\n",
    "        counter = None\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        while counter is not 0:\n",
    "            batch_X, batch_Y, counter = self.get_train_mini_batch()\n",
    "            loss += self.nn.compute_loss(batch_X, batch_Y)* batch_Y.shape[0]\n",
    "            pred = self.nn.predict(batch_X)\n",
    "            accuracy += np.equal(pred,np.argmax(batch_Y, axis=1)).sum()\n",
    "        \n",
    "        loss /= self.Y_train.shape[0]\n",
    "        accuracy /= self.Y_train.shape[0]\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    def compute_test(self):\n",
    "        counter = None\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        while counter is not 0:\n",
    "            batch_X, batch_Y, counter = self.get_test_mini_batch()\n",
    "            loss += self.nn.compute_loss(batch_X, batch_Y)* batch_Y.shape[0]\n",
    "            pred = self.nn.predict(batch_X)\n",
    "            accuracy += np.equal(pred,np.argmax(batch_Y, axis=1)).sum()\n",
    "        \n",
    "        loss /= self.Y_test.shape[0]\n",
    "        accuracy /= self.Y_test.shape[0]\n",
    "\n",
    "        return loss, accuracy\n",
    "        \n",
    "        \n",
    "    def get_test_mini_batch(self):\n",
    "\n",
    "        if (self.test_batch_counter + 1)* self.batch_size < self.X_test.shape[0]:\n",
    "#             batch_X = self.X_test.tocsc()[self.test_batch_counter* self.batch_size: (self.test_batch_counter+1)*self.batch_size,:]\n",
    "            batch_X = self.X_test[self.test_batch_counter* self.batch_size: (self.test_batch_counter+1)*self.batch_size,:]\n",
    "            batch_Y = self.Y_test[self.test_batch_counter* self.batch_size: (self.test_batch_counter+1)*self.batch_size,:]\n",
    "            self.test_batch_counter += 1\n",
    "\n",
    "        else:\n",
    "#             batch_X = self.X_test.tocsc()[self.test_batch_counter* self.batch_size:,: ]\n",
    "            batch_X = self.X_test[self.test_batch_counter* self.batch_size:,: ]\n",
    "            batch_Y = self.Y_test[self.test_batch_counter* self.batch_size:,: ]\n",
    "            self.test_batch_counter = 0\n",
    "\n",
    "        return batch_X, batch_Y, self.test_batch_counter\n",
    "\n",
    "    def get_train_mini_batch(self):\n",
    "\n",
    "        if (self.train_batch_counter + 1)* self.batch_size < self.X_train.shape[0]:\n",
    "#             batch_X = self.X_train.tocsc()[self.train_batch_counter* self.batch_size: (self.train_batch_counter+1)*self.batch_size,:]\n",
    "            batch_X = self.X_train[self.train_batch_counter* self.batch_size: (self.train_batch_counter+1)*self.batch_size,:]\n",
    "            batch_Y = self.Y_train[self.train_batch_counter* self.batch_size: (self.train_batch_counter+1)*self.batch_size,:]\n",
    "            self.train_batch_counter += 1\n",
    "\n",
    "        else:\n",
    "#             batch_X = self.X_train.tocsc()[self.train_batch_counter* self.batch_size:,: ]\n",
    "            batch_X = self.X_train[self.train_batch_counter* self.batch_size:,: ]\n",
    "            batch_Y = self.Y_train[self.train_batch_counter* self.batch_size:,: ]\n",
    "            self.train_batch_counter = 0\n",
    "\n",
    "        return batch_X, batch_Y, self.train_batch_counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, ind, parent, data_partition, dim_in, num_classes,\n",
    "                 layers, activation, regularization,batch_size=100):\n",
    "\n",
    "        self.ind = ind\n",
    "        self.parent = parent\n",
    "        self.nn = NeuralNet(dim_in, num_classes, layers, activation, regularization)\n",
    "        self.X_train, self.Y_train = unpack_data(data_partition)\n",
    "        self.data_size = self.X_train.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_counter = 0\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        ind = list(range(self.data_size))\n",
    "        shuffle(ind)\n",
    "        self.X_train = self.X_train[ind,:]\n",
    "        self.Y_train = self.Y_train[ind,:]\n",
    "\n",
    "    def reset_batch_counter(self):\n",
    "        self.batch_counter = 0\n",
    "    \n",
    "    def get_delayed_params(self):\n",
    "        \n",
    "        if self.parent.max_delay is not 0:\n",
    "            delay = np.random.randint(0, self.parent.max_delay)\n",
    "            delayed_params = self.parent.queue.sample(delay)\n",
    "        else:\n",
    "            delay = 0\n",
    "            delayed_params = self.parent.queue.sample(0)\n",
    "        \n",
    "        truth = delayed_params[1][0]==self.parent.nn.bias[0]\n",
    "#         print(truth)\n",
    "#         if sum(truth[0])==10:\n",
    "#             print(\"delay=\",delay, self.parent.queue.len)\n",
    "        \n",
    "        return delayed_params\n",
    "        \n",
    "    def compute_mini_batch_gradients(self, X, Y):\n",
    "        \n",
    "        params = self.get_delayed_params()\n",
    "#         assert(params == [self.parent.nn.weights, self.parent.nn.bias])\n",
    "        grads = self.nn.grad(X, Y, params)\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def get_next_mini_batch(self):\n",
    "        \n",
    "        if (self.batch_counter + 1)* self.batch_size < self.X_train.shape[0]:\n",
    "            batch_X = self.X_train[self.batch_counter* self.batch_size: (self.batch_counter+1)*self.batch_size, :]\n",
    "            batch_Y = self.Y_train[self.batch_counter* self.batch_size: (self.batch_counter+1)*self.batch_size, :]\n",
    "            self.batch_counter += 1\n",
    "\n",
    "        else:\n",
    "            batch_X = self.X_train[self.batch_counter* self.batch_size:,: ]\n",
    "            batch_Y = self.Y_train[self.batch_counter* self.batch_size:,: ]\n",
    "            self.batch_counter = 0\n",
    "            # end of epoch\n",
    "            # do shuffling at the end of each epoch?\n",
    "            \n",
    "            \n",
    "        return batch_X, batch_Y\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    def __init__(self, dim_in, num_classes, layers, activation='softmax', regularization=1e-3):\n",
    "        self.dim_in = dim_in\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        self.weights = dict.fromkeys(range(self.num_layers + 1))\n",
    "        self.bias = dict.fromkeys(range(self.num_layers + 1))\n",
    "        self.gradients_W = dict.fromkeys(range(self.num_layers + 1))\n",
    "        self.gradients_b = dict.fromkeys(range(self.num_layers + 1))\n",
    "        self.regularization = regularization\n",
    "        \n",
    "        if activation == 'softmax':\n",
    "            self.activation = softmax\n",
    "        else:\n",
    "            self.activation = ReLu\n",
    "        \n",
    "        prev_dim = dim_in\n",
    "        for key, value in enumerate(layers):\n",
    "            self.weights[key] = np.random.randn(prev_dim, value)\n",
    "            self.bias[key] = np.random.randn(1, value)\n",
    "            prev_dim = value\n",
    "        \n",
    "        self.weights[self.num_layers] = np.random.randn(prev_dim, num_classes)\n",
    "        self.bias[self.num_layers] = np.random.randn(1, num_classes)\n",
    "        self.cache = []\n",
    "        \n",
    "\n",
    "    def calculate_forward_layer(self, W, b, X):\n",
    "        \n",
    "#         Z = np.dot(X, W) + b\n",
    "#         print(X.shape, W.shape, b.shape)\n",
    "        Z = X.dot(W) + b\n",
    "        Z = self.activation(Z)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    \n",
    "    def assign_weights(self, delayed_params):\n",
    "        \n",
    "        weights, bias = delayed_params\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    \n",
    "    def forward(self,X):\n",
    "        self.cache = []\n",
    "        temp = X\n",
    "        for layer in range(self.num_layers):\n",
    "            temp = self.calculate_forward_layer(self.weights[layer], self.bias[layer], temp)\n",
    "            self.cache.append(temp)\n",
    "            if layer == self.num_layers - 1:\n",
    "                break\n",
    "        \n",
    "#         print(self.weights[self.num_layers].shape, self.bias[self.num_layers].shape, temp.shape)\n",
    "#         out = np.dot(temp, self.weights[self.num_layers]) + self.bias[self.num_layers]\n",
    "        out = temp.dot(self.weights[self.num_layers]) + self.bias[self.num_layers]\n",
    "        out = softmax(out)\n",
    "        self.cache.append(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def cross_entropy_loss(self, Y, Z):\n",
    "\n",
    "        loss = -np.sum(Y*np.log(Z))\n",
    "        \n",
    "        return loss/Y.shape[0]\n",
    "        \n",
    "\n",
    "    def grad(self, X, Y, delayed_params):\n",
    "        \n",
    "        \n",
    "        self.assign_weights(delayed_params)\n",
    "        self.forward(X)\n",
    "        \n",
    "        # for squared loss, as well as cross entropy loss\n",
    "        delta = self.cache[-1] - Y\n",
    "        \n",
    "        for layer in reversed(range(self.num_layers+1)):\n",
    "        \n",
    "            if layer is not 0:\n",
    "            \n",
    "                temp_grad_W = (self.cache[layer-1].T).dot(delta)\n",
    "                temp_grad_b = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "                self.gradients_W[layer] = temp_grad_W + self.regularization * self.weights[layer]\n",
    "                self.gradients_b[layer] = temp_grad_b\n",
    "\n",
    "                delta = delta.dot(self.weights[layer].T) * (1- np.power(self.cache[layer-1], 2))\n",
    "\n",
    "            else:\n",
    "                temp_grad_W = (X.T).dot(delta)\n",
    "                temp_grad_b = np.sum(delta, axis=0, keepdims=True)\n",
    "\n",
    "                self.gradients_W[layer] = temp_grad_W + self.regularization * self.weights[layer]\n",
    "                self.gradients_b[layer] = temp_grad_b            \n",
    "            \n",
    "        return self.gradients_W, self.gradients_b\n",
    "            \n",
    "        \n",
    "    def compute_loss(self, batch_X, batch_Y):\n",
    "        Z = self.forward(batch_X)\n",
    "        loss = self.cross_entropy_loss(batch_Y,Z)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        Z = self.forward(X)\n",
    "        return np.argmax(Z, axis=1)\n",
    "        \n",
    "    def accuracy(self, pred, Y):\n",
    "        \n",
    "        print(np.equal(pred,Y).sum()/pred.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = ParamServer(num_workers=10, max_delay=100, dim_in=784, num_classes=10,\n",
    "                     layers=[], activation=\"softmax\",learning_rate=1e-2, num_iter=10000,\n",
    "                     regularization=0, dataset=\"mnist\", num_epochs=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56000, 784), (14000, 784))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server.X_train.shape, server.X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 784) (14000, 10)\n",
      "(14000, 784) (14000, 10)\n",
      "(14000, 784) (14000, 10)\n",
      "(14000, 784) (14000, 10)\n"
     ]
    }
   ],
   "source": [
    "for _,w in server.workers.items():\n",
    "    print(w.X_train.shape, w.Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 0 train_loss 2.55 test_loss 2.35 test_accuracy 0.32\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 1 train_loss 2.39 test_loss 2.22 test_accuracy 0.31\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 2 train_loss 2.24 test_loss 2.14 test_accuracy 0.45\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 3 train_loss 2.15 test_loss 2.09 test_accuracy 0.51\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 4 train_loss 2.09 test_loss 2.03 test_accuracy 0.63\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 5 train_loss 2.04 test_loss 1.99 test_accuracy 0.66\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 6 train_loss 1.99 test_loss 1.94 test_accuracy 0.69\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 7 train_loss 1.94 test_loss 1.9 test_accuracy 0.68\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 8 train_loss 1.9 test_loss 1.85 test_accuracy 0.7\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 9 train_loss 1.86 test_loss 1.81 test_accuracy 0.72\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 10 train_loss 1.82 test_loss 1.77 test_accuracy 0.72\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 11 train_loss 1.78 test_loss 1.74 test_accuracy 0.72\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 12 train_loss 1.74 test_loss 1.7 test_accuracy 0.73\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 13 train_loss 1.7 test_loss 1.67 test_accuracy 0.75\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 14 train_loss 1.67 test_loss 1.63 test_accuracy 0.75\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 15 train_loss 1.64 test_loss 1.6 test_accuracy 0.76\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 16 train_loss 1.61 test_loss 1.57 test_accuracy 0.76\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 17 train_loss 1.58 test_loss 1.55 test_accuracy 0.76\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 18 train_loss 1.55 test_loss 1.52 test_accuracy 0.77\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 19 train_loss 1.52 test_loss 1.49 test_accuracy 0.77\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 20 train_loss 1.49 test_loss 1.47 test_accuracy 0.77\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 21 train_loss 1.47 test_loss 1.44 test_accuracy 0.78\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 22 train_loss 1.44 test_loss 1.42 test_accuracy 0.78\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 23 train_loss 1.42 test_loss 1.4 test_accuracy 0.78\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 24 train_loss 1.4 test_loss 1.37 test_accuracy 0.79\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 25 train_loss 1.38 test_loss 1.35 test_accuracy 0.79\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 26 train_loss 1.35 test_loss 1.33 test_accuracy 0.79\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 27 train_loss 1.33 test_loss 1.31 test_accuracy 0.8\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 28 train_loss 1.31 test_loss 1.3 test_accuracy 0.8\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 29 train_loss 1.3 test_loss 1.28 test_accuracy 0.8\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 30 train_loss 1.28 test_loss 1.26 test_accuracy 0.8\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 31 train_loss 1.26 test_loss 1.24 test_accuracy 0.8\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 32 train_loss 1.24 test_loss 1.23 test_accuracy 0.8\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 33 train_loss 1.23 test_loss 1.21 test_accuracy 0.8\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 34 train_loss 1.21 test_loss 1.2 test_accuracy 0.81\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 35 train_loss 1.2 test_loss 1.18 test_accuracy 0.81\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 36 train_loss 1.18 test_loss 1.17 test_accuracy 0.81\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 37 train_loss 1.17 test_loss 1.15 test_accuracy 0.81\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 38 train_loss 1.15 test_loss 1.14 test_accuracy 0.81\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 39 train_loss 1.14 test_loss 1.13 test_accuracy 0.81\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 40 train_loss 1.13 test_loss 1.12 test_accuracy 0.81\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 41 train_loss 1.12 test_loss 1.1 test_accuracy 0.82\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 42 train_loss 1.1 test_loss 1.09 test_accuracy 0.82\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 43 train_loss 1.09 test_loss 1.08 test_accuracy 0.82\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 44 train_loss 1.08 test_loss 1.07 test_accuracy 0.82\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 45 train_loss 1.07 test_loss 1.06 test_accuracy 0.82\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 46 train_loss 1.06 test_loss 1.05 test_accuracy 0.82\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 47 train_loss 1.05 test_loss 1.04 test_accuracy 0.82\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 48 train_loss 1.04 test_loss 1.03 test_accuracy 0.82\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "epoch 49 train_loss 1.03 test_loss 1.02 test_accuracy 0.82\n",
      "39.40140748023987\n"
     ]
    }
   ],
   "source": [
    "from time import time as time\n",
    "t1=time()\n",
    "logs = server.train()\n",
    "t2=time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f82df0f60b8>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82df11cd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "gen_error = [abs(logs[i][\"train_loss\"] - logs[i][\"test_loss\"]) for i in range(server.num_epochs)]\n",
    "train_error = [logs[i][\"train_loss\"] for i in range(server.num_epochs)]\n",
    "test_error = [logs[i][\"test_loss\"] for i in range(server.num_epochs)]\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0].plot(gen_error, 'r')\n",
    "# ax.plot(gen_error, 'r')\n",
    "ax[1].plot(train_error, 'g')\n",
    "ax[1].plot(test_error, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f82e9a650f0>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82e9ba8908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_accuracy = [logs[i][\"train_acc\"] for i in range(server.num_epochs)]\n",
    "test_accuracy = [logs[i][\"test_acc\"] for i in range(server.num_epochs)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_accuracy,'g')\n",
    "ax.plot(test_accuracy,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "\n",
    "# X_train, Y_train = load_svmlight_file(\"data/a9a/a9a.txt\")\n",
    "# X_test, Y_test = load_svmlight_file(\"data/a9a/a9a.t\")\n",
    "# X_test = sparse.hstack((X_test, np.zeros((X_test.shape[0],1))))\n",
    "\n",
    "mnist = fetch_mldata('MNIST original', data_home=\"data/\")\n",
    "target = np.eye(10)[mnist.target.astype(\"int\")]\n",
    "mnist.data = preprocessing.normalize(mnist.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = list(range(mnist.data.shape[0]))\n",
    "shuffle(ind)\n",
    "X_train = mnist.data[ind[:int(np.floor(target.shape[0]*0.8))],:]\n",
    "Y_train = target[ind[:int(np.floor(target.shape[0]*0.8))],:]\n",
    "X_test = mnist.data[ind[int(np.floor(target.shape[0]*0.8)):],:]\n",
    "Y_test = target[ind[int(np.floor(target.shape[0]*0.8)):],:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(server.X_test, server.Y_test.argmax(axis=1)), server.compute_test(), server.compute_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=clf.predict_log_proba(X_test)\n",
    "Y_test += 1\n",
    "Y_test /= 2\n",
    "Y_test = np.eye(2)[Y_test.astype(\"int\")]\n",
    "\n",
    "loss = -np.sum(Y_test*Z)\n",
    "loss/= Y_test.shape[0]\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = clf.predict_log_proba(X_test)\n",
    "loss = -np.sum(Y_test*Z)\n",
    "loss/= Y_test.shape[0]\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.compute_test(), server.compute_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(server.nn.weights[0].nbytes + server.nn.weights[1].nbytes + server.nn.weights[2].nbytes)/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(server.nn.weights[0].nbytes , server.nn.weights[1].nbytes , server.nn.weights[2].nbytes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
