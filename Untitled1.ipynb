{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from numpy.random import randint\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "# # dtype = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# # N is batch size; D_in is input dimension;\n",
    "# # H is hidden dimension; D_out is output dimension.\n",
    "# N, D_in, H, D_out = 32, 784, 100, 10\n",
    "\n",
    "# # Create random Tensors to hold input and outputs.\n",
    "# # Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# # with respect to these Tensors during the backward pass.\n",
    "# x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "# y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# # Create random Tensors for weights.\n",
    "# # Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# # respect to these Tensors during the backward pass.\n",
    "# w1 = torch.zeros(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "# w2 = torch.zeros(H, D_out, device=device, dtype=dtype, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "# #         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "# #         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "# #         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(784, 1000)\n",
    "#         self.fc2 = nn.Linear(1000, 100)\n",
    "#         self.fc3 = nn.Linear(100,10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "# #         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "# #         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "# #         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "# #         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc3(x)\n",
    "\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# model = Net()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# model = Net()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.306609\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 2.321286\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.291904\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.335230\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.383439\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.284001\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.311998\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.325318\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.340397\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.310720\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-3e072f67fc9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         print(t, loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/torch/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/torch/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(10):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "#         data = data.reshape(32,784)\n",
    "\n",
    "#         y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "        y_pred = model(x)\n",
    "\n",
    "        criterion = torch.nn.NLLLoss()\n",
    "#         criterion = torch.nn.CrossEntropyLoss()\n",
    "        loss = criterion(y_pred, target)\n",
    "#         loss = (y_pred - y).pow(2).sum()\n",
    "#         print(t, loss.item())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             w1 -= learning_rate * w1.grad\n",
    "#             w2 -= learning_rate * w2.grad\n",
    "\n",
    "#             # Manually zero the gradients after updating weights\n",
    "#             w1.grad.zero_()\n",
    "#             w2.grad.zero_()\n",
    "\n",
    "        if batch_idx % 1000==0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                t, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
